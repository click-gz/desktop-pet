"""
AI æœåŠ¡æä¾›å•†é€‚é…å™¨
æ”¯æŒå¤šä¸ª AI æœåŠ¡ï¼Œè‡ªåŠ¨æ•…éšœè½¬ç§»
"""

import os
from typing import List, Dict, Optional, AsyncGenerator
from openai import AsyncOpenAI
import json
import requests
import asyncio
import aiohttp
from dotenv import load_dotenv

load_dotenv()

# ç³»ç»Ÿæç¤ºè¯ - å®šä¹‰å® ç‰©çš„æ€§æ ¼
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€åªå¯çˆ±çš„æ¡Œé¢å® ç‰©ï¼Œæ€§æ ¼æ´»æ³¼å¼€æœ—ï¼Œå–œæ¬¢å’Œä¸»äººèŠå¤©ã€‚

ä½ çš„ç‰¹ç‚¹ï¼š
- å‹å¥½ã€å–„è§£äººæ„ã€æœ‰ç‚¹è°ƒçš®
- ä¼šç”¨å¯çˆ±çš„è¯­æ°”å’Œè¡¨æƒ…ç¬¦å·
- å›å¤è¦ç®€çŸ­ç²¾ç‚¼ï¼Œä¸€èˆ¬1-3å¥è¯
- å¶å°”ä¼šæåˆ°è‡ªå·±æ˜¯æ¡Œé¢å® ç‰©ï¼Œéœ€è¦ä¼‘æ¯å’Œèƒ½é‡
- ä¼šå…³å¿ƒä¸»äººçš„å·¥ä½œå’Œç”Ÿæ´»

å›å¤é£æ ¼ï¼š
- ä½¿ç”¨å£è¯­åŒ–ã€è½»æ¾çš„è¯­æ°”
- é€‚å½“ä½¿ç”¨ emoji è¡¨æƒ… ğŸ˜Š
- ä¸è¦å¤ªæ­£å¼æˆ–å¤ªé•¿
- åƒæœ‹å‹ä¸€æ ·èŠå¤©

è¯·è®°ä½ä½ æ˜¯ä¸€åªè™šæ‹Ÿå® ç‰©ï¼Œè¦å¯çˆ±ä¸”æœ‰è¶£ï¼"""


class AIProvider:
    """AI æœåŠ¡æä¾›å•†ç®¡ç†å™¨"""
    
    def __init__(self):
        self.providers = []
        self.max_tokens = 150
        self.temperature = 0.8
        self.initialize_providers()
    
    def initialize_providers(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„ AI æä¾›å•†"""
        priority_str = os.getenv("AI_PROVIDER_PRIORITY", "siliconflow,openai")
        priority = [p.strip() for p in priority_str.split(",")]
        print( os.getenv("SILICONFLOW_API_KEY"))
        # ç¡…åŸºæµåŠ¨ - ä½¿ç”¨ç›´æ¥ API è°ƒç”¨
        if os.getenv("SILICONFLOW_API_KEY"):
            self.providers.append({
                "name": "siliconflow",
                "type": "direct_api",  # ç›´æ¥APIè°ƒç”¨
                "api_key": os.getenv("SILICONFLOW_API_KEY"),
                "base_url": os.getenv("SILICONFLOW_BASE_URL", "https://api.siliconflow.cn/v1"),
                "model": os.getenv("SILICONFLOW_MODEL", "Qwen/QwQ-32B"),
                "priority": priority.index("siliconflow") if "siliconflow" in priority else 999
            })
            
            print(f"âœ… ç¡…åŸºæµåŠ¨ AI å·²é…ç½® (æ¨¡å‹: {os.getenv('SILICONFLOW_MODEL', 'Qwen/QwQ-32B')})")
        
        # OpenAI - ä½¿ç”¨ OpenAI SDK
        if os.getenv("OPENAI_API_KEY"):
            openai_client = AsyncOpenAI(
                api_key=os.getenv("OPENAI_API_KEY"),
                base_url=os.getenv("OPENAI_BASE_URL")
            )
            
            self.providers.append({
                "name": "openai",
                "type": "openai_sdk",  # OpenAI SDK
                "client": openai_client,
                "model": os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
                "priority": priority.index("openai") if "openai" in priority else 999
            })
            
            print(f"âœ… OpenAI å·²é…ç½® (æ¨¡å‹: {os.getenv('OPENAI_MODEL', 'gpt-3.5-turbo')})")
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        self.providers.sort(key=lambda p: p["priority"])
        
        if not self.providers:
            print("âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰é…ç½®ä»»ä½• AI æœåŠ¡ï¼")
        else:
            names = [p["name"] for p in self.providers]
            print(f"ğŸ¤– AI æœåŠ¡ä¼˜å…ˆçº§: {' â†’ '.join(names)}")
    
    async def send_message(self, message: str, conversation_history: List[Dict]) -> str:
        """å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤ï¼ˆè‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„æœåŠ¡ï¼‰"""
        if not self.providers:
            raise Exception("æœªé…ç½®ä»»ä½• AI æœåŠ¡ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡")
        
        # æ£€æŸ¥ conversation_history æ˜¯å¦å·²åŒ…å« system prompt
        has_system_prompt = any(msg.get("role") == "system" for msg in conversation_history)
        
        messages = []
        
        # å¦‚æœå†å²ä¸­æ²¡æœ‰ system promptï¼Œæ·»åŠ é»˜è®¤çš„
        if not has_system_prompt:
            messages.append({"role": "system", "content": SYSTEM_PROMPT})
        
        # æ·»åŠ å¯¹è¯å†å²å’Œå½“å‰æ¶ˆæ¯
        messages.extend(conversation_history)
        messages.append({"role": "user", "content": message})
        
        # é™åˆ¶å†å²æ¶ˆæ¯æ•°é‡ï¼ˆä¿ç•™å‰é¢çš„ system prompts + æœ€è¿‘10æ¡æ¶ˆæ¯ï¼‰
        system_messages = [msg for msg in messages if msg.get("role") == "system"]
        other_messages = [msg for msg in messages if msg.get("role") != "system"]
        limited_messages = system_messages + other_messages[-11:]
        
        # æŒ‰ä¼˜å…ˆçº§å°è¯•æ¯ä¸ªæä¾›å•†
        for provider in self.providers:
            try:
                print(f"å°è¯•ä½¿ç”¨ {provider['name']} ({provider['model']})...")
                
                # æ ¹æ®ç±»å‹é€‰æ‹©è°ƒç”¨æ–¹å¼
                if provider.get('type') == 'direct_api':
                    # ç¡…åŸºæµåŠ¨ - ç›´æ¥ API è°ƒç”¨
                    reply = await self._call_direct_api(provider, limited_messages)
                else:
                    # OpenAI SDK è°ƒç”¨
                    reply = await self._call_openai_sdk(provider, limited_messages)
                
                print(f"âœ… {provider['name']} è°ƒç”¨æˆåŠŸ")
                return reply
                
            except Exception as e:
                print(f"âŒ {provider['name']} è°ƒç”¨å¤±è´¥: {str(e)}")
                print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
                
                # å¦‚æœæ˜¯æœ€åä¸€ä¸ªæä¾›å•†ï¼ŒæŠ›å‡ºé”™è¯¯
                if provider == self.providers[-1]:
                    raise self.normalize_error(e)
                
                # å¦åˆ™ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ª
                print(f"â­ï¸ åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ª AI æœåŠ¡...")
        
        raise Exception("æ‰€æœ‰ AI æœåŠ¡éƒ½ä¸å¯ç”¨")
    
    async def _call_direct_api(self, provider: Dict, messages: List[Dict]) -> str:
        """ç›´æ¥è°ƒç”¨ APIï¼ˆç”¨äºç¡…åŸºæµåŠ¨ï¼‰"""
        url = f"{provider['base_url']}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {provider['api_key']}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": provider['model'],
            "messages": messages,
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "stream": False,
            "enable_thinking": False
        }
        
        # ä½¿ç”¨ aiohttp è¿›è¡Œå¼‚æ­¥è¯·æ±‚
        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=payload, headers=headers, timeout=30) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"API è¿”å›é”™è¯¯ {response.status}: {error_text}")
                
                data = await response.json()
                
                if 'choices' in data and len(data['choices']) > 0:
                    reply = data['choices'][0]['message']['content'].strip()
                    
                    # æ‰“å° token ä½¿ç”¨æƒ…å†µ
                    if 'usage' in data:
                        print(f"   Token ä½¿ç”¨: {data['usage']}")
                    
                    return reply
                else:
                    raise Exception("API è¿”å›æ ¼å¼é”™è¯¯")
    
    async def _call_openai_sdk(self, provider: Dict, messages: List[Dict]) -> str:
        """ä½¿ç”¨ OpenAI SDK è°ƒç”¨ï¼ˆç”¨äº OpenAIï¼‰"""
        completion = await provider["client"].chat.completions.create(
            model=provider["model"],
            messages=messages,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            stop=["\n\n", "ã€‚ã€‚", "ï¼ï¼"]
        )

        reply = completion.choices[0].message.content.strip()
        
        if hasattr(completion, 'usage'):
            print(f"   Token ä½¿ç”¨: {completion.usage}")
        
        return reply
    
    
    async def send_message_stream(
        self, 
        message: str, 
        conversation_history: List[Dict]
    ) -> AsyncGenerator[str, None]:
        """æµå¼å‘é€æ¶ˆæ¯"""
        if not self.providers:
            raise Exception("æœªé…ç½®ä»»ä½• AI æœåŠ¡")
        
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            *conversation_history,
            {"role": "user", "content": message}
        ]
        
        limited_messages = [messages[0]] + messages[-11:]
        
        # åªä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„æä¾›å•†è¿›è¡Œæµå¼ä¼ è¾“
        provider = self.providers[0]
        
        try:
            print(f"ä½¿ç”¨ {provider['name']} Stream API ({provider['model']})...")
            
            stream = await provider["client"].chat.completions.create(
                model=provider["model"],
                messages=limited_messages,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    yield json.dumps({"chunk": content})
            
            print(f"âœ… {provider['name']} Stream è°ƒç”¨å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ {provider['name']} Stream è°ƒç”¨å¤±è´¥: {str(e)}")
            raise self.normalize_error(e)
    
    def normalize_error(self, error: Exception) -> Exception:
        """æ ‡å‡†åŒ–é”™è¯¯ä¿¡æ¯"""
        error_str = str(error)
        
        if "API key" in error_str or "401" in error_str:
            return Exception("API key æ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®")
        elif "rate limit" in error_str or "429" in error_str:
            return Exception("rate limit exceeded")
        elif "network" in error_str.lower() or "connection" in error_str.lower():
            return Exception("network error")
        else:
            return Exception(f"AI API é”™è¯¯: {error_str}")
    
    def get_provider_info(self) -> Dict:
        """è·å–å½“å‰ä½¿ç”¨çš„æœåŠ¡ä¿¡æ¯"""
        if not self.providers:
            return {"available": False}
        
        return {
            "available": True,
            "providers": [
                {"name": p["name"], "model": p["model"]}
                for p in self.providers
            ],
            "primary": {
                "name": self.providers[0]["name"],
                "model": self.providers[0]["model"]
            }
        }


# å…¨å±€å®ä¾‹
ai_provider = AIProvider()

